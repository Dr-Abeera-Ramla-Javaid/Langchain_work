{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Abeera-Ramla-Javaid/Langchain_work/blob/main/modules/module-0/basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
      "metadata": {
        "id": "ef597741-3211-4ecc-92f7-f58023ee237e"
      },
      "source": [
        "# LangChain Academy\n",
        "\n",
        "Welcome to LangChain Academy!\n",
        "\n",
        "## Context\n",
        "\n",
        "At LangChain, we aim to make it easy to build LLM applications. One type of LLM application you can build is an agent. There’s a lot of excitement around building agents because they can automate a wide range of tasks that were previously impossible.\n",
        "\n",
        "In practice though, it is incredibly difficult to build systems that reliably execute on these tasks. As we’ve worked with our users to put agents into production, we’ve learned that more control is often necessary. You might need an agent to always call a specific tool first or use different prompts based on its state.\n",
        "\n",
        "To tackle this problem, we’ve built [LangGraph](https://langchain-ai.github.io/langgraph/) — a framework for building agent and multi-agent applications. Separate from the LangChain package, LangGraph’s core design philosophy is to help developers add better precision and control into agent workflows, suitable for the complexity of real-world systems.\n",
        "\n",
        "## Course Structure\n",
        "\n",
        "The course is structured as a set of modules, with each module focused on a particular theme related to LangGraph. You will see a folder for each module, which contains a series of notebooks. A video will accompany each notebook to help walk through the concepts, but the notebooks are also stand-alone, meaning that they contain explanations and can be viewed independent of the videos. Each module folder also contains a `studio` folder, which contains a set of graphs that can be loaded into [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio), our IDE for building LangGraph applications.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Before you begin, please follow the instructions in the `README` to create an environment and install dependencies.\n",
        "\n",
        "## Chat models\n",
        "\n",
        "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which do a few things take a sequence of messages as inputs and return chat messages as outputs. LangChain does not host any Chat Models, rather we rely on third party integrations. [Here](https://python.langchain.com/v0.2/docs/integrations/chat/) is a list of 3rd party chat model integrations within LangChain! By default, the course with use [GEMINI_API_KEY](https://ai.google.dev/gemini-api/docs/api-key/) because it is both popular and performant. As noted, please ensure that you have an `GEMINI_API_KEY`.\n",
        "\n",
        "Let's check that your `GEMINI_API_KEY` is set and, if not, you will be asked to enter it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -q langchain_google_genai langchain_core langchain_community tavily-python"
      ],
      "metadata": {
        "id": "4htYPR6U3NRR"
      },
      "id": "4htYPR6U3NRR",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-b2jKyJp1QdS"
      },
      "id": "-b2jKyJp1QdS"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model='gemini-1.5-flash',\n",
        "    api_key= GEMINI_API_KEY,\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "ipsiAKX73DYX"
      },
      "id": "ipsiAKX73DYX",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke('Hello Gemini')\n",
        "result"
      ],
      "metadata": {
        "id": "f6yS0Gd23LBT",
        "outputId": "9c4de192-e446-4d41-9b6c-a1a2519496af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "f6yS0Gd23LBT",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello there!  How can I help you today?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-a9513a6d-84a1-437c-8f67-681ad1217848-0', usage_metadata={'input_tokens': 3, 'output_tokens': 12, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import name\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "# msg= HumanMessage(content=\"hello gemini\", name= \"Abeera\")\n",
        "\n",
        "messages =[\n",
        "     HumanMessage(content=\"hello gemini\", name= \"Abeera\"),\n",
        "     AIMessage(content='Hello there! How can I help you today?\\n', name= 'AI Assistant'),\n",
        "     HumanMessage(content= \"what is LANGCHAIN?\", name= \"Abeera\"),\n",
        "     AIMessage(content=\"LangChain is a framework for developing applications powered by language models.  It's designed to make it easier to build applications that use LLMs (Large Language Models) effectively, addressing some of the limitations and complexities involved.\"),\n",
        "     HumanMessage(content= \"How Can I Learn?\", name= 'Abeera')\n",
        "     ]\n",
        "\n",
        "\n",
        "llm.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-tCt750BW6Q",
        "outputId": "f94b0d3e-70fa-4c34-e351-83f45438b3d9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='There are several ways you can learn LangChain:\\n\\n**1. Official Documentation:** The best place to start is the official LangChain documentation.  It\\'s well-structured and provides comprehensive tutorials and examples.  Look for their website and their GitHub repository.\\n\\n**2. Tutorials and Examples:**  Many tutorials and examples are available online. Search for \"LangChain tutorial\" on YouTube, Google, or other platforms.  Look for tutorials that cover specific aspects you\\'re interested in, such as integrating with specific LLMs or building particular types of applications.\\n\\n**3. GitHub Repository:** Explore the LangChain GitHub repository.  You can find the source code, contribute to the project, and see how different components are implemented.  This is a great way to learn by example and understand the inner workings of the framework.\\n\\n**4. Community and Forums:** Engage with the LangChain community.  There are likely forums, Discord servers, or other online communities where you can ask questions, share your progress, and learn from others.  Searching for \"LangChain community\" will likely turn up relevant resources.\\n\\n**5. Build Projects:** The most effective way to learn is by doing.  Start with small projects and gradually increase the complexity.  Try building a simple chatbot, a question-answering system, or a document summarizer using LangChain.  This hands-on experience will solidify your understanding.\\n\\n**6. Blogs and Articles:** Many blogs and articles cover LangChain and its applications.  Searching for relevant keywords will yield numerous resources.\\n\\n**7. Courses (Future Possibility):** While not as prevalent yet, dedicated courses on LangChain might emerge as its popularity grows.  Keep an eye out for online learning platforms offering such courses.\\n\\n\\nRemember to start with the basics and gradually move towards more advanced concepts.  Focus on understanding the core components of LangChain and how they interact with each other.  Good luck!\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-14ec7818-7910-4943-bb40-fe01ffac0b20-0', usage_metadata={'input_tokens': 74, 'output_tokens': 396, 'total_tokens': 470, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "id": "r-tCt750BW6Q"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "# tool= TavilySearchResults(max_results=2)\n",
        "tavily_search= TavilySearchResults(max_results=3)\n",
        "search_docs = tavily_search.invoke('what is langGraph')"
      ],
      "metadata": {
        "id": "nDwxVxEMG5P2"
      },
      "execution_count": 20,
      "outputs": [],
      "id": "nDwxVxEMG5P2"
    },
    {
      "cell_type": "code",
      "source": [
        "search_docs"
      ],
      "metadata": {
        "id": "k8VzYD4xJbvO",
        "outputId": "47ca9fa0-f066-4c2c-af12-d9695f712d7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://langchain-ai.github.io/langgraph/',\n",
              "  'content': 'LangGraph is a framework for creating stateful, multi-actor applications with LLMs, using cycles, controllability, and persistence. Learn how to use LangGraph with examples, integration with LangChain, and streaming support.'},\n",
              " {'url': 'https://towardsdatascience.com/from-basics-to-advanced-exploring-langgraph-e8c1cf4db787',\n",
              "  'content': \"LangGraph is a low-level framework that offers extensive customisation options, allowing you to build precisely what you need. Since LangGraph is built on top of LangChain, it's seamlessly integrated into its ecosystem, making it easy to leverage existing tools and components. However, there are areas where LangGrpah could be improved:\"},\n",
              " {'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
              "  'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents in a structured and efficient manner. Learn how to install, use, and customize LangGraph to build complex, scalable, and flexible multi-agent systems.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "id": "k8VzYD4xJbvO"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}