{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Abeera-Ramla-Javaid/Langchain_work/blob/main/modules/module-3/1_streaming_interruption.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
      "metadata": {
        "id": "319adfec-2d0a-49f2-87f9-275c4a32add2"
      },
      "source": [
        "# Streaming\n",
        "\n",
        "## Review\n",
        "\n",
        "In module 2, covered a few ways to customize graph state and memory.\n",
        "\n",
        "We built up to a Chatbot with external memory that can sustain long-running conversations.\n",
        "\n",
        "## Goals\n",
        "\n",
        "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways.\n",
        "\n",
        "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
      "metadata": {
        "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_google_genai langgraph_sdk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
      "metadata": {
        "id": "70d7e41b-c6ba-4e47-b645-6c110bede549"
      },
      "source": [
        "## Streaming\n",
        "\n",
        "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
        "\n",
        "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5b430d92-f595-4322-a56e-06de7485daa8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b430d92-f595-4322-a56e-06de7485daa8",
        "outputId": "0e6caac4-8cd0-4bb9-8909-a8b367704445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: GOOGLE_API_KEY=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\n",
            "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\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "%env GOOGLE_API_KEY = {userdata.get('GEMINI_API_KEY')}\n",
        "\n",
        "import os\n",
        "print(os.environ[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9b2a5d0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b2a5d0a",
        "outputId": "4029eb9a-170a-4be1-bb5c-3f2ddf28f02b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LANGCHAIN_API_KEY=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\n"
          ]
        }
      ],
      "source": [
        "%env LANGCHAIN_API_KEY = {userdata.get('LANGCHAIN_API_KEY')}\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0682fc",
      "metadata": {
        "id": "4d0682fc"
      },
      "source": [
        "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
        "outputId": "42964255-6b29-43c1-b641-ff4d5d476925"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "# LLM\n",
        "model: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash\")\n",
        "\n",
        "# State\n",
        "class State(MessagesState):\n",
        "    summary: str\n",
        "\n",
        "# Define the logic to call the model\n",
        "def call_model(state: State, config: RunnableConfig):\n",
        "\n",
        "    # Get summary if it exists\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # If there is summary, then we add it\n",
        "    if summary:\n",
        "\n",
        "        # Add summary to system message\n",
        "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
        "\n",
        "        # Append summary to any newer messages\n",
        "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "\n",
        "    else:\n",
        "        messages = state[\"messages\"]\n",
        "\n",
        "    response = model.invoke(messages, config)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def summarize_conversation(state: State):\n",
        "\n",
        "    # First, we get any existing summary\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # Create our summarization prompt\n",
        "    if summary:\n",
        "\n",
        "        # A summary already exists\n",
        "        summary_message = (\n",
        "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
        "            \"Extend the summary by taking into account the new messages above:\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        summary_message = \"Create a summary of the conversation above:\"\n",
        "\n",
        "    # Add prompt to our history\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
        "\n",
        "# Determine whether to end or summarize the conversation\n",
        "def should_continue(state: State):\n",
        "\n",
        "    \"\"\"Return the next node to execute.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # If there are more than six messages, then we summarize the conversation\n",
        "    if len(messages) > 6:\n",
        "        return \"summarize_conversation\"\n",
        "\n",
        "    # Otherwise we can just end\n",
        "    return END\n",
        "\n",
        "# Define a new graph\n",
        "workflow: StateGraph = StateGraph(State)\n",
        "workflow.add_node(\"conversation\", call_model)\n",
        "workflow.add_node(summarize_conversation)\n",
        "\n",
        "# Set the entrypoint as conversation\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
        "workflow.add_edge(\"summarize_conversation\", END)\n",
        "\n",
        "# Compile\n",
        "memory: MemorySaver = MemorySaver()\n",
        "graph: CompiledStateGraph = workflow.compile(checkpointer=memory)\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f847a787-b301-488c-9b58-cba9f389f55d",
      "metadata": {
        "id": "f847a787-b301-488c-9b58-cba9f389f55d"
      },
      "source": [
        "### Streaming full state\n",
        "\n",
        "Now, let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
        "\n",
        "`.stream` and `.astream` are sync and async methods for streaming back results.\n",
        "\n",
        "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
        "\n",
        "* `values`: This streams the full state of the graph after each node is called.\n",
        "* `updates`: This streams updates to the state of the graph after each node is called.\n",
        "\n",
        "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
        "\n",
        "Let's look at `stream_mode=\"updates\"`.\n",
        "\n",
        "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
        "\n",
        "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Start conversation\n",
        "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Abeera\")]}, config, stream_mode=\"updates\"):\n",
        "    print(chunk)"
      ],
      "metadata": {
        "id": "hwWI636D7tCG",
        "outputId": "1b7bc5c5-953c-426c-b387-29040a98f4ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "id": "hwWI636D7tCG",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ChatGoogleGenerativeAIError",
          "evalue": "Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key not valid. Please pass a valid API key.\"\n]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;31m# Do not retry for these errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    831\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             _retry_error_helper(\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgument\u001b[0m: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key not valid. Please pass a valid API key.\"\n]",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mChatGoogleGenerativeAIError\u001b[0m               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-78fdb5171aa4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Start conversation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hi! I'm Abeera\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"updates\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   1648\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mrun_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, writer)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;31m# if successful, end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e6a0c9ae16d7>\u001b[0m in \u001b[0;36mcall_model\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         return cast(\n\u001b[1;32m    285\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    785\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         flattened_outputs = [\n\u001b[1;32m    645\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 results.append(\n\u001b[0;32m--> 633\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    634\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    852\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mtool_choice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtool_choice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         )\n\u001b[0;32m--> 946\u001b[0;31m         response: GenerateContentResponse = _chat_with_retry(\n\u001b[0m\u001b[1;32m    947\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(generation_method, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_chat_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_post_retry_check_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RetryCallState\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_run_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgument\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             raise ChatGoogleGenerativeAIError(\n\u001b[0m\u001b[1;32m    191\u001b[0m                 \u001b[0;34mf\"Invalid argument provided to Gemini: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             ) from e\n",
            "\u001b[0;31mChatGoogleGenerativeAIError\u001b[0m: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key not valid. Please pass a valid API key.\"\n]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
      "metadata": {
        "id": "0c4882e9-07dd-4d70-866b-dfc530418cad"
      },
      "source": [
        "Let's now just print the state update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c859c777-cb12-4682-9108-6b367e597b81",
      "metadata": {
        "id": "c859c777-cb12-4682-9108-6b367e597b81"
      },
      "outputs": [],
      "source": [
        "# Start conversation\n",
        "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Abeera\")]}, config, stream_mode=\"updates\"):\n",
        "    chunk['conversation'][\"messages\"].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "583bf219-6358-4d06-ae99-c40f43569fda",
      "metadata": {
        "id": "583bf219-6358-4d06-ae99-c40f43569fda"
      },
      "source": [
        "Now, we can see `stream_mode=\"values\"`.\n",
        "\n",
        "This is the `full state` of the graph after the `conversation` node is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
        "outputId": "f14d2d4c-dcf5-442c-ee28-39a5b5bf8526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi! I'm Lance\n",
            "---------------------------------------------------------------------------\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi! I'm Lance\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Lance! Nice to meet you. What can I do for you today?\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Start conversation, again\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "# Start conversation\n",
        "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
        "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
        "    for m in event['messages']:\n",
        "        m.pretty_print()\n",
        "    print(\"---\"*25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
      "metadata": {
        "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7"
      },
      "source": [
        "### Streaming tokens\n",
        "\n",
        "We often want to stream more than graph state.\n",
        "\n",
        "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
        "\n",
        "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
        "\n",
        "Each event is a dict with a few keys:\n",
        "\n",
        "* `event`: This is the type of event that is being emitted.\n",
        "* `name`: This is the name of event.\n",
        "* `data`: This is the data associated with the event.\n",
        "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
        "\n",
        "Let's have a look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
        "outputId": "d5a47429-65cc-4c2c-a32c-f9a08b8d0c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node: . Type: on_chain_start. Name: LangGraph\n",
            "Node: __start__. Type: on_chain_start. Name: __start__\n",
            "Node: __start__. Type: on_chain_end. Name: __start__\n",
            "Node: conversation. Type: on_chain_start. Name: conversation\n",
            "Node: conversation. Type: on_chat_model_start. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_end. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chain_start. Name: _write\n",
            "Node: conversation. Type: on_chain_end. Name: _write\n",
            "Node: conversation. Type: on_chain_start. Name: should_continue\n",
            "Node: conversation. Type: on_chain_end. Name: should_continue\n",
            "Node: conversation. Type: on_chain_stream. Name: conversation\n",
            "Node: conversation. Type: on_chain_end. Name: conversation\n",
            "Node: . Type: on_chain_stream. Name: LangGraph\n",
            "Node: . Type: on_chain_end. Name: LangGraph\n"
          ]
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
        "\n",
        "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
        "\n",
        "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
        "\n",
        "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
      "metadata": {
        "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d"
      },
      "source": [
        "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
        "\n",
        "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
        "\n",
        "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
        "outputId": "31bdc2e2-ce2c-48ec-bb76-6183e9ab0ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'chunk': AIMessageChunk(content='## The', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': []}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 2, 'total_tokens': 13})}\n",
            "{'chunk': AIMessageChunk(content=' San Francisco 49ers: A Legacy of Success and Grit\\n\\nThe San Francisco 4', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 21, 'total_tokens': 32})}\n",
            "{'chunk': AIMessageChunk(content='9ers are one of the most storied franchises in NFL history, boasting a', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 37, 'total_tokens': 48})}\n",
            "{'chunk': AIMessageChunk(content=\" rich legacy of success, iconic players, and passionate fans. Here's a glimpse into their story:\\n\\n**Early Days and Rise to Greatness:**\\n\\n\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 68, 'total_tokens': 79})}\n",
            "{'chunk': AIMessageChunk(content='* **Founded in 1946:** The 49ers were one of the original 11 teams that formed the All-America Football Conference (', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 101, 'total_tokens': 112})}\n",
            "{'chunk': AIMessageChunk(content='AAFC). They joined the NFL in 1950.\\n* **The \"Golden Era\":** The 1980s marked a golden age for the franchise. Led by legendary coach Bill Walsh, the 49ers', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 151, 'total_tokens': 162})}\n",
            "{'chunk': AIMessageChunk(content=' won four Super Bowls in a span of 13 years (1982, 1985, 1989, 1995).\\n* **Iconic Players:** This era featured legendary players like', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 199, 'total_tokens': 210})}\n",
            "{'chunk': AIMessageChunk(content=' Joe Montana, Jerry Rice, Steve Young, Ronnie Lott, and Charles Haley, who are considered some of the greatest to ever play the game.\\n\\n**Recent Years and Current Status:**\\n\\n* **Post-Super Bowl Success:** After a period of relative decline, the 49ers experienced a resurgence in the late 2', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 265, 'total_tokens': 276})}\n",
            "{'chunk': AIMessageChunk(content='010s and early 2020s. They reached the Super Bowl in 2019 and 2020, losing both times.\\n* **Current Roster:** The 49ers currently have a young and talented roster, featuring players like Trey Lance, Deebo Samuel,', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 329, 'total_tokens': 340})}\n",
            "{'chunk': AIMessageChunk(content=\" George Kittle, and Fred Warner.\\n* **Head Coach Kyle Shanahan:** The team is led by head coach Kyle Shanahan, known for his innovative offensive schemes and ability to develop young talent.\\n\\n**Key Facts:**\\n\\n* **Home Stadium:** Levi's Stadium in Santa Clara, California.\\n*\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 393, 'total_tokens': 404})}\n",
            "{'chunk': AIMessageChunk(content=\" **Mascot:** Sourdough Sam.\\n* **Colors:** Red, gold, and black.\\n* **Rivals:** Seattle Seahawks, Los Angeles Rams, and Arizona Cardinals.\\n\\n**The 49ers are more than just a football team; they are a symbol of San Francisco's spirit and a testament to the\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 460, 'total_tokens': 471})}\n",
            "{'chunk': AIMessageChunk(content=' enduring legacy of the game.** Their fans are known for their passion and unwavering support, making every game a thrilling experience. \\n\\n**To delve deeper, you can explore:**\\n\\n* **Official website:** www.49ers.com\\n* **Wikipedia:** https://en.wikipedia.org/wiki/San_Francisco_', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 528, 'total_tokens': 539})}\n",
            "{'chunk': AIMessageChunk(content=\"49ers\\n* **NFL.com:** https://www.nfl.com/teams/san-francisco-49ers\\n\\nWhether you're a lifelong fan or just starting to learn about the team, the San Francisco 49ers offer a compelling story of success, resilience, and the enduring power of the\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 595, 'total_tokens': 606})}\n",
            "{'chunk': AIMessageChunk(content=' game. \\n', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-ce8c38fe-970a-4e17-83ca-8230f92246ce', usage_metadata={'input_tokens': 11, 'output_tokens': 597, 'total_tokens': 608})}\n"
          ]
        }
      ],
      "source": [
        "node_to_stream = 'conversation'\n",
        "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
        "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
        "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
        "    # Get chat model tokens from a particular node\n",
        "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
        "        print(event[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
      "metadata": {
        "id": "226e569a-76c3-43d8-8f89-3ae687efde1c"
      },
      "source": [
        "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
        "outputId": "997579e3-aaa7-4455-cc9f-7e39bcb8addd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The| San Francisco 49ers: A Legacy of Excellence\n",
            "\n",
            "The San Francisco 49|ers are one of the most storied franchises in NFL history, boasting a rich| legacy of success and a passionate fan base. Here's a glimpse into their world:\n",
            "\n",
            "**History:**\n",
            "\n",
            "* **Founded:** 1946,| originally as the \"Golden Gate 49ers\" in the All-America Football Conference (AAFC).\n",
            "* **Joined NFL:** 1950,| after the AAFC folded.\n",
            "* **Home Stadium:** Levi's Stadium (Santa Clara, California) since 2014. Previously played at Candlestick Park (1971-2014) and Kezar| Stadium (1946-1970).\n",
            "* **Team Colors:** Gold, scarlet red, and black.\n",
            "* **Mascot:** \"Sourdough Sam,\" a sourdough bread baker.\n",
            "\n",
            "**Achievements:**\n",
            "\n",
            "*| **Super Bowl Championships (5):** Super Bowl XVI (1982), XIX (1985), XXIII (1989), XXIX (1995), and XXXIV (2000).\n",
            "* **NFL Championships (3):** 1957, 19|81, 1989.\n",
            "* **NFC Championships (16):** 1971, 1981, 1982, 1984, 1985, 1988, 1989, 1990|, 1992, 1994, 1997, 2011, 2012, 2013, 2019, and 2022.\n",
            "* **Hall of Famers:**  Numerous, including Joe Montana,| Jerry Rice, Steve Young, Ronnie Lott, Deion Sanders, and many more.\n",
            "\n",
            "**Recent Years:**\n",
            "\n",
            "* The 49ers have experienced a recent resurgence, making it to the Super Bowl in 2020 and the NFC Championship game in 2022. \n",
            "* **Current| Head Coach:** Kyle Shanahan\n",
            "* **Notable Players:**  George Kittle, Deebo Samuel, Nick Bosa, Fred Warner, Trey Lance, Christian McCaffrey.\n",
            "\n",
            "**Culture:**\n",
            "\n",
            "* Known for their strong defense and innovative offensive schemes.\n",
            "* The team has a strong connection to the Bay Area and| its diverse communities.\n",
            "* The \"Faithful\" fanbase is renowned for their passion and dedication.\n",
            "\n",
            "**The 49ers are a team rich in history and tradition, with a bright future ahead. Their commitment to excellence and loyal fanbase make them one of the most exciting and beloved teams in the NFL.** \n",
            "|"
          ]
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
        "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
        "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
        "    # Get chat model tokens from a particular node\n",
        "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
        "        data = event[\"data\"]\n",
        "        print(data[\"chunk\"].content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
      "metadata": {
        "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db"
      },
      "source": [
        "### Streaming with LangGraph API\n",
        "\n",
        "--\n",
        "\n",
        "** DISCLAIMER**\n",
        "\n",
        "*Running Studio currently requires a Mac. If you are not using a Mac, then skip this step.*\n",
        "\n",
        "*Also, if you are running this notebook in CoLab, then skip this step.*\n",
        "\n",
        "--\n",
        "\n",
        "The LangGraph API [has first class support for streaming](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#streaming).\n",
        "\n",
        "Let's load our `agent` in the Studio UI, which uses `module-3/studio/agent.py` set in `module-3/studio/langgraph.json`.\n",
        "\n",
        "The LangGraph API serves as the back-end for Studio.\n",
        "\n",
        "We can interact directly with the LangGraph API via the LangGraph SDK.\n",
        "\n",
        "We just need to get the URL for the local deployment from Studio.\n",
        "\n",
        "![Screenshot 2024-08-27 at 2.20.34 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf8943c3d4df239cbf0f_streaming2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "079c2ad6",
      "metadata": {
        "id": "079c2ad6"
      },
      "outputs": [],
      "source": [
        "from langgraph_sdk import get_client\n",
        "\n",
        "# Replace this with the URL of your own deployed graph\n",
        "URL = \"http://localhost:56091\"\n",
        "client = get_client(url=URL)\n",
        "\n",
        "# Search all hosted graphs\n",
        "assistants = await client.assistants.search()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32",
      "metadata": {
        "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32"
      },
      "source": [
        "Let's [stream `values`](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/), like before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
      "metadata": {
        "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
        "outputId": "bb18d884-8366-4158-e696-437661418558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StreamPart(event='metadata', data={'run_id': '1ef6a3d0-41eb-66f4-a311-8ebdfa1b281f'})\n",
            "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '345c67cf-c958-4f89-b787-540fc025080c', 'example': False}]})\n",
            "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '345c67cf-c958-4f89-b787-540fc025080c', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-88179a6d-eb1e-4953-ac42-0b533b6d76f6', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}]})\n",
            "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '345c67cf-c958-4f89-b787-540fc025080c', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-88179a6d-eb1e-4953-ac42-0b533b6d76f6', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '4dd5ce10-ac0b-4a91-b34b-c35109dcbf29', 'tool_call_id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'artifact': None, 'status': 'success'}]})\n",
            "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '345c67cf-c958-4f89-b787-540fc025080c', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-88179a6d-eb1e-4953-ac42-0b533b6d76f6', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '4dd5ce10-ac0b-4a91-b34b-c35109dcbf29', 'tool_call_id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'artifact': None, 'status': 'success'}, {'content': 'The result of multiplying 2 and 3 is 6.', 'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-b5862486-a25f-48fc-9a03-a8506a6692a8', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]})\n"
          ]
        }
      ],
      "source": [
        "# Create a new thread\n",
        "thread = await client.threads.create()\n",
        "# Input message\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "async for event in client.runs.stream(thread[\"thread_id\"],\n",
        "                                      assistant_id=\"agent\",\n",
        "                                      input={\"messages\": [input_message]},\n",
        "                                      stream_mode=\"values\"):\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
      "metadata": {
        "id": "556dc7fd-1cae-404f-816a-f13d772b3b14"
      },
      "source": [
        "The streamed objects have:\n",
        "\n",
        "* `event`: Type\n",
        "* `data`: State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b735aa-139c-45a3-a850-63519c0004f0",
      "metadata": {
        "id": "57b735aa-139c-45a3-a850-63519c0004f0",
        "outputId": "45769b8a-1f9c-463e-ced2-857b323ea49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================\n",
            "content='Multiply 2 and 3' additional_kwargs={'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'example': False} id='f51807de-6b99-4da4-a798-26cf59d16412'\n",
            "=========================\n",
            "content='' additional_kwargs={'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_imZHAw7kvMR2ZeKaQVSlj25C', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'example': False, 'invalid_tool_calls': [], 'usage_metadata': None} id='run-fa4ab1c6-274d-4be5-8c4a-a6411c7c35cc' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_imZHAw7kvMR2ZeKaQVSlj25C', 'type': 'tool_call'}]\n",
            "=========================\n",
            "content='6' additional_kwargs={'additional_kwargs': {}, 'response_metadata': {}, 'status': 'success'} name='multiply' id='3e7bbfb6-aa82-453a-969c-9c753fbd1d74' tool_call_id='call_imZHAw7kvMR2ZeKaQVSlj25C'\n",
            "=========================\n",
            "content='The result of multiplying 2 and 3 is 6.' additional_kwargs={'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'example': False, 'invalid_tool_calls': [], 'usage_metadata': None} id='run-e8e0d672-cfb2-42be-850a-345df3718f69'\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import convert_to_messages\n",
        "thread = await client.threads.create()\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
        "    messages = event.data.get('messages',None)\n",
        "    if messages:\n",
        "        print(convert_to_messages(messages)[-1])\n",
        "    print('='*25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a555d186-27be-4ddf-934c-895a3105035d",
      "metadata": {
        "id": "a555d186-27be-4ddf-934c-895a3105035d"
      },
      "source": [
        "There are some new streaming mode that are only supported via the API.\n",
        "\n",
        "For example, we can [use `messages` mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) to better handle the above case!\n",
        "\n",
        "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
        "\n",
        "All events emitted using `messages` mode have two attributes:\n",
        "\n",
        "* `event`: This is the name of the event\n",
        "* `data`: This is data associated with the event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
      "metadata": {
        "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
        "outputId": "fdf211ba-bcac-4028-c073-6c0078e42fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "metadata\n",
            "messages/complete\n",
            "messages/metadata\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/complete\n",
            "messages/complete\n",
            "messages/metadata\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/complete\n"
          ]
        }
      ],
      "source": [
        "thread = await client.threads.create()\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "async for event in client.runs.stream(thread[\"thread_id\"],\n",
        "                                      assistant_id=\"agent\",\n",
        "                                      input={\"messages\": [input_message]},\n",
        "                                      stream_mode=\"messages\"):\n",
        "    print(event.event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
      "metadata": {
        "id": "8de2f1ea-b232-43fc-af7a-320efce83381"
      },
      "source": [
        "We can see a few events:\n",
        "\n",
        "* `metadata`: metadata about the run\n",
        "* `messages/complete`: fully formed message\n",
        "* `messages/partial`: chat model tokens\n",
        "\n",
        "You can dig further into the types [here](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages).\n",
        "\n",
        "Now, let's show how to stream these messages.\n",
        "\n",
        "We'll define a helper function for better formatting of the tool calls in messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
      "metadata": {
        "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
        "outputId": "db5a7af9-caa8-4251-aa35-52d25a2c3e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata: Run ID - 1ef6a3da-687f-6253-915a-701de5327165\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
            "Response Metadata: Finish Reason - tool_calls\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "AI: The\n",
            "--------------------------------------------------\n",
            "AI: The result\n",
            "--------------------------------------------------\n",
            "AI: The result of\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying \n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and \n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3 is\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3 is \n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3 is 6\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3 is 6.\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3 is 6.\n",
            "Response Metadata: Finish Reason - stop\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "thread = await client.threads.create()\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "\n",
        "def format_tool_calls(tool_calls):\n",
        "    \"\"\"\n",
        "    Format a list of tool calls into a readable string.\n",
        "\n",
        "    Args:\n",
        "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
        "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if tool_calls:\n",
        "        formatted_calls = []\n",
        "        for call in tool_calls:\n",
        "            formatted_calls.append(\n",
        "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
        "            )\n",
        "        return \"\\n\".join(formatted_calls)\n",
        "    return \"No tool calls\"\n",
        "\n",
        "async for event in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id=\"agent\",\n",
        "    input={\"messages\": [input_message]},\n",
        "    stream_mode=\"messages\",):\n",
        "\n",
        "    # Handle metadata events\n",
        "    if event.event == \"metadata\":\n",
        "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Handle partial message events\n",
        "    elif event.event == \"messages/partial\":\n",
        "        for data_item in event.data:\n",
        "            # Process user messages\n",
        "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
        "                print(f\"Human: {data_item['content']}\")\n",
        "            else:\n",
        "                # Extract relevant data from the event\n",
        "                tool_calls = data_item.get(\"tool_calls\", [])\n",
        "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
        "                content = data_item.get(\"content\", \"\")\n",
        "                response_metadata = data_item.get(\"response_metadata\", {})\n",
        "\n",
        "                if content:\n",
        "                    print(f\"AI: {content}\")\n",
        "\n",
        "                if tool_calls:\n",
        "                    print(\"Tool Calls:\")\n",
        "                    print(format_tool_calls(tool_calls))\n",
        "\n",
        "                if invalid_tool_calls:\n",
        "                    print(\"Invalid Tool Calls:\")\n",
        "                    print(format_tool_calls(invalid_tool_calls))\n",
        "\n",
        "                if response_metadata:\n",
        "                    finish_reason = response_metadata.get(\"finish_reason\", \"N/A\")\n",
        "                    print(f\"Response Metadata: Finish Reason - {finish_reason}\")\n",
        "\n",
        "        print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
      "metadata": {
        "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}